{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjjCK1E13KGH"
   },
   "source": [
    "# Import, Install and Mount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UHBzLWJQPw0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import randint\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from boruta import BorutaPy\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f3dn-A0Yvbcm"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvF_QzfUQPxF"
   },
   "source": [
    "# Data Preparation Class\n",
    "- Read Data\n",
    "- Standard Deviation Filter\n",
    "- Split Function\n",
    "- Smote Upsampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIrp9jamvpuW"
   },
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "    \n",
    "  # Read Data\n",
    "    def read_data(self, path, nrows, usecols):\n",
    "        data = pd.read_csv(path, nrows=nrows, usecols=usecols)\n",
    "        data.index = data.iloc[:,0]\n",
    "        data.drop(columns = \"Unnamed: 0\", inplace = True)\n",
    "        data.columns = [(re.sub('\\.\\d+', '', gene)) for gene in data.columns]\n",
    "        return data\n",
    "  \n",
    "  # Filter with Standard Deviation Threshold\n",
    "    def X_and_y(self, data, threshold):\n",
    "        #data.describe()\n",
    "        X = data.drop(columns = 'label')\n",
    "        X_sd = X.loc[:, X.std() > threshold]\n",
    "        y = data[[\"label\"]]\n",
    "        return X_sd, y\n",
    "  \n",
    "  # Train Test Split data\n",
    "    def split(self, X, y, test_size):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=self.seed)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "  \n",
    "  # Upsample unbalanced data\n",
    "    def smote_up(self, X_train, y_train):\n",
    "        #print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train['label']==1)))\n",
    "        #print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train['label']==0)))\n",
    "\n",
    "        sm = SMOTE(random_state=self.seed)\n",
    "        X_train_smote, y_train_smote = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "        #print('After OverSampling, the shape of train_X: {}'.format(X_train_smote.shape))\n",
    "        #print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n",
    "\n",
    "        #print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_smote==1)))\n",
    "        #print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_smote==0)))\n",
    "        column_names = X_train.columns\n",
    "\n",
    "        # Make dataframe again\n",
    "        X_train_smote = pd.DataFrame(X_train_smote, columns=column_names)\n",
    "        y_train_smote = pd.DataFrame(y_train_smote, columns=['label'])\n",
    "\n",
    "        return X_train_smote, y_train_smote\n",
    "    \n",
    "    def bulbasaur(self, path, threshold = 2, nrows = None, usecols = None):\n",
    "        data = self.read_data(path, nrows, usecols)\n",
    "        X, y = self.X_and_y(data, threshold)\n",
    "        X_train, X_test, y_train, y_test = self.split(X, y, 0.3)\n",
    "        X_train_smote, y_train_smote = self.smote_up(X_train, y_train)\n",
    "        return X_train_smote, y_train_smote, X_test, y_test\n",
    "    \n",
    "dataprep = DataPrep(1888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJRYDoNk3El8"
   },
   "source": [
    "# Feature Selection Class\n",
    "- Boruta\n",
    "- RFE\n",
    "- Gradient Boost Classifier\n",
    "- Elastic Net\n",
    "- Visualisation \n",
    "- Call Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3eyDQGHiCts"
   },
   "outputs": [],
   "source": [
    "class FeatureSelection:\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "    \n",
    "    def rfe(self, X_train, y_train, X_test, y_test, n_features = 300, step = 0.3, kernel = \"linear\"):\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination - step < 1 is a percentage. Returns selected features.\n",
    "        \"\"\"\n",
    "        # Create RFE\n",
    "        estimator = SVR(kernel=kernel)\n",
    "        selector = RFE(estimator, n_features_to_select = n_features, step=step)\n",
    "        selector = selector.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "        \n",
    "        # Print Accuracy\n",
    "        print('Accuracy of RFE: {:.3f}'.format(selector.score(X_test, y_test)))\n",
    "        \n",
    "        # Add features and feature importance to dictionary\n",
    "        selected_features = X_train.columns[selector.support_].tolist()\n",
    "        \n",
    "        feature_importances = [1 for x in range(len(selected_features))]\n",
    "    \n",
    "        dictionary = {\"Recursive Feature Elimination\":[selected_features, feature_importances]}\n",
    "        return dictionary\n",
    "  \n",
    "  \n",
    "    def gradient_boost_classifier(self, X_train, y_train, X_test, y_test, n_features = 300):\n",
    "        \"\"\"\n",
    "        Gradient Boost Classifier with feature importance selection.\n",
    "        \"\"\"\n",
    "        # Create Gradient Boost Classifier \n",
    "        new = GradientBoostingClassifier(learning_rate=0.1, n_estimators=250, max_depth=4, \\\n",
    "                                         min_samples_split=8, min_samples_leaf=1,\\\n",
    "                                         subsample=0.75, random_state=self.seed)\n",
    "        new.fit(X_train,y_train)\n",
    "        predictors = list(X_train)\n",
    "        feat_imp = pd.Series(new.feature_importances_, predictors).sort_values(ascending=False)[:n_features]\n",
    "        \n",
    "        pred = new.predict(X_test)\n",
    "        \n",
    "        # Print accuracy\n",
    "        print('Accuracy of GBM: {:.3f}'.format(new.score(X_test, y_test)))\n",
    "    \n",
    "        # Add features and feature importance to dictionary\n",
    "        importances = new.feature_importances_\n",
    "        genes = X_test.columns\n",
    "        \n",
    "        selected_features_df = pd.DataFrame(importances, index = genes).sort_values(0, ascending = False).head(n_features)\n",
    "        selected_features = selected_features_df.index.tolist()\n",
    "        feature_importances = selected_features_df.iloc[:,0].tolist()\n",
    "        \n",
    "        dictionary = {\"Gradient Boost Classifier\":[selected_features, feature_importances]}\n",
    "        return dictionary\n",
    "    \n",
    "    def elastic_net(self, X_train_smote, y_train_res, X_test, y_test, alpha=0.01, l1_ratio=0.5, n_features=300):\n",
    "        clf = ElasticNet(random_state=self.seed, alpha=alpha, l1_ratio=l1_ratio)\n",
    "        clf.fit(X_train_smote, y_train_res)\n",
    "\n",
    "        clf.pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"Accuracy of Elastic Net: {:.3f}\".format(clf.score(X_test, y_test)))\n",
    "        #print(\"Elastic net mean squared error: {:.3f}\".format(mean_squared_error(y_test, clf.pred)))\n",
    "\n",
    "        ft_imp = pd.DataFrame(clf.coef_, index=X_train_smote.columns)\n",
    "        ft_sort = ft_imp.sort_values(0, ascending=False)\n",
    "        imp_coef = pd.concat([ft_sort.head(int(n_features/2)), ft_sort.tail(int(n_features/2))])\n",
    "\n",
    "        selected_features = imp_coef.index.tolist()\n",
    "        feature_importances = imp_coef.iloc[:,0].tolist()\n",
    "\n",
    "        dictionary = {\"Elastic Net\": [selected_features, feature_importances]}\n",
    "\n",
    "        return dictionary \n",
    "  \n",
    "    def boruta_tree(self, X_train_smote, y_train_res, X_test, y_test):\n",
    "\n",
    "        for _ in range(1):\n",
    "\n",
    "            from sklearn.metrics import f1_score # import again because it works like that :)\n",
    "\n",
    "            # Random Forests for Boruta\n",
    "            rf_boruta = RandomForestClassifier(n_jobs=-1, random_state=self.seed)\n",
    "\n",
    "            # Perform Boruta\n",
    "            boruta = BorutaPy(rf_boruta, n_estimators='auto', verbose=0,\n",
    "                          alpha=0.005, max_iter=30, perc=100, random_state=self.seed)\n",
    "            boruta.fit(X_train_smote.values, y_train_res)\n",
    "\n",
    "            # Select features and fit Logistic Regression\n",
    "\n",
    "            cols = X_train_smote.columns[boruta.support_]\n",
    "            X_train_smote = X_train_smote[cols]\n",
    "            est_boruta = LogisticRegression(random_state=self.seed)\n",
    "            est_boruta.fit(X_train_smote, y_train_res)\n",
    "\n",
    "            scores = cross_val_score(est_boruta, X_train_smote, y_train_res, cv=5)\n",
    "\n",
    "            print(\"Accuracy of Boruta: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "        # Random Forest for extracting features\n",
    "\n",
    "        X_filtered = X_train_smote[cols]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = self.seed)\n",
    "        rf.fit(X_filtered, y_train_res)\n",
    "        rf_pred = rf.predict(X_test[cols])\n",
    "        print(\"Accuracy of Boruta Tree: {:.3f}\".format(accuracy_score(y_test, rf_pred)))\n",
    "\n",
    "        feature_names = X_filtered.columns\n",
    "        rf_coeff = pd.DataFrame({\"feature\": feature_names,\"coefficient\": rf.feature_importances_})\n",
    "        rf_coeff_top = rf_coeff.sort_values(by=\"coefficient\",ascending=False).head(300).set_index(\"feature\")\n",
    "\n",
    "        selected_features = rf_coeff_top.index.tolist()\n",
    "        feature_importances = rf_coeff_top.coefficient.tolist()\n",
    "\n",
    "        dictionary = {\"Boruta Tree\": [selected_features, feature_importances]}\n",
    "\n",
    "        return dictionary\n",
    "  \n",
    "    def visualize_selected_features(self, X, y, selected_features):\n",
    "        \"\"\"\n",
    "        Visualize features\n",
    "        \"\"\"\n",
    "        X_selected = X[selected_features]\n",
    "\n",
    "        import seaborn as sns\n",
    "        fig = plt.figure(figsize = (20, 25))\n",
    "        j = 0\n",
    "        for i in X_selected.columns:\n",
    "            plt.subplot(14, 10, j+1)\n",
    "            j += 1\n",
    "            sns.distplot(X_selected[i][data[\"label\"]==0], color='g', label = '0')\n",
    "            sns.distplot(X_selected[i][data[\"label\"]==1], color='r', label = '1')\n",
    "            plt.legend(loc='best')\n",
    "        fig.suptitle('Target Data Analysis')\n",
    "        fig.tight_layout()\n",
    "        fig.subplots_adjust(top=0.95)\n",
    "        plt.show() \n",
    "        \n",
    "        \n",
    "    def call_methods(self, X_train, y_train, X_test, y_test):\n",
    "        method1 = self.gradient_boost_classifier(X_train, y_train, X_test, y_test, n_features = 300)\n",
    "        method2 = self.rfe(X_train, y_train, X_test, y_test, kernel = \"linear\")\n",
    "        method3 = self.elastic_net(X_train, y_train, X_test, y_test, alpha=0.01, l1_ratio=0.5, n_features = 300)\n",
    "        method4 = self.boruta_tree(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        return { **method1, **method2, **method3, **method4 }\n",
    "        \n",
    "# Instantiate Class Object\n",
    "FS = FeatureSelection(1888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Combine Intogen Genes with TCGA\n",
    "- Compare Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \n",
    "    def add_intogen_to_dict(self, path, dict_list):\n",
    "        \"\"\"\n",
    "        Add a dictionary with Intogen genes and corresponding Mutation Count\n",
    "        \"\"\"\n",
    "        #path = \"Data/Intogen_Data/Lung_Adenocarcinoma_LUAD_TCGA.tsv\"\n",
    "        intogen_df = pd.read_csv(path, delimiter = \"\\t\")\n",
    "        importances = intogen_df[\"MUTS_PAM_SAMPLES\"].to_list()\n",
    "        intogen_genes = {\"Intogen\":[intogen_df[\"GENE\"].to_list(), importances]}\n",
    "        dict_list = {**dict_list, **intogen_genes}\n",
    "        \n",
    "        return dict_list\n",
    "    \n",
    "    def results(self, dict_list):\n",
    "        \"\"\"\n",
    "        Store results in a csv\n",
    "        Includes: Count for each method, feature importance, intogen counts\n",
    "        \"\"\"\n",
    "        row_names = []\n",
    "        column_names = []\n",
    "\n",
    "        # Create dataframe with all viable genes from all method results\n",
    "        for method, selected_features in dict_list.items():\n",
    "            \n",
    "            for feature in selected_features[0]:\n",
    "                row_names.append(feature)\n",
    "\n",
    "            row_names = list(set(row_names))\n",
    "            column_names.append(method)\n",
    "\n",
    "        results = pd.DataFrame(columns = column_names, index = row_names)\n",
    "        results.fillna(0, inplace = True)\n",
    "\n",
    "        # Add a one where the method selected the corresponding feature\n",
    "        for method, selected_features in dict_list.items():\n",
    "            for feature in selected_features[0]:\n",
    "                results.at[feature, method] = 1\n",
    "\n",
    "\n",
    "        # Create Column with total count\n",
    "        results['Total Count'] = results[list(results.columns)].sum(axis=1)\n",
    "        results.sort_values(by = \"Total Count\", ascending = False, inplace = True)\n",
    "        \n",
    "        # Add Importance Columns\n",
    "        for method, selected_features in dict_list.items():\n",
    "            additional = pd.DataFrame({\"Importances: \" + method:selected_features[1]}, index = selected_features[0])\n",
    "            results = results.join(additional, on=results.index, how='left')\n",
    "        \n",
    "        # Clean dataframe\n",
    "        results.fillna(0, inplace = True)\n",
    "        \n",
    "        ordered_columns = ['Total Count', 'Intogen', 'Gradient Boost Classifier', 'Recursive Feature Elimination', 'Elastic Net', 'Boruta Tree',  'Importances: Gradient Boost Classifier','Importances: Recursive Feature Elimination','Importances: Elastic Net', 'Importances: Boruta Tree','Importances: Intogen']\n",
    "        results = results[ordered_columns]\n",
    "        return results\n",
    "    \n",
    "    def final_results(self, path, path_intogen, nrows = 200, usecols = [x for x in range(100)], threshold = 3):\n",
    "        \"\"\"\n",
    "        Puts everything together\n",
    "        \"\"\"\n",
    "        X_train, y_train, X_test, y_test = dataprep.bulbasaur(path, threshold, nrows = nrows, usecols = usecols)\n",
    "        dict_list = FS.call_methods(X_train, y_train, X_test, y_test)\n",
    "        dict_list = evaluation.add_intogen_to_dict(path_intogen, dict_list)\n",
    "        df = evaluation.results(dict_list)\n",
    "        #df.to_csv(\"Breast_results.csv\")\n",
    "        return df\n",
    "\n",
    "    def iterate_trough_cancers(self, path_list, path_intogen_list, nrows, usecols, threshold = 2.5):\n",
    "        \"\"\"\n",
    "        Iterate through all cancer data\n",
    "        \"\"\"\n",
    "        for path, path_intogen in zip(path_list, path_intogen_list):\n",
    "            start = time.time()\n",
    "            name = re.sub(\"^.+\\/Chunk_\", \"\", path)\n",
    "            filepath = 'Output/Result_{}'.format(name)\n",
    "\n",
    "            print('Evaluating {}'.format(path))      \n",
    "\n",
    "            results = evaluation.final_results(path, path_intogen, nrows = nrows, usecols = usecols, threshold = threshold)            \n",
    "            results.to_csv(filepath)\n",
    "\n",
    "            print('Finished in {:.1f} min\\n'.format((time.time() - start) / 60))\n",
    "\n",
    "evaluation = Evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_list = [\"Data/Expression_Data/['Breast']_['Primary Tumor', 'Normal Tissue']_1.0.csv\",\n",
    "            \"Data/Expression_Data/['Colon']_['Primary Tumor', 'Normal Tissue']_1.0.csv\",\n",
    "            \"Data/Expression_Data/['Skin']_['Primary Tumor', 'Normal Tissue']_1.0.csv\",\n",
    "            \"Data/Expression_Data/['Thyroid', 'Thyroid Gland']_['Primary Tumor', 'Normal Tissue']_1.0.csv\",\n",
    "            \"Data/Expression_Data/['Lung']_['Lung Adenocarcinoma', 'Lung']_['Primary Tumor', 'Normal Tissue']_1.0.csv\",\n",
    "            \"Data/Expression_Data/['Lung']_['Lung Squamous Cell Carcinoma', 'Lung']_['Primary Tumor', 'Normal Tissue']_1.0.csv\"]\n",
    "\n",
    "path_intogen_list = [\"Data/Intogen_Data/Breast_Invasive_BRCA_TCGA.tsv\",\n",
    "                    \"Data/Intogen_Data/Colon_Adenocarcinoma_COREAD_TCGA.tsv\",\n",
    "                    \"Data/Intogen_Data/Skin_Cutaneous_Melanoma_CM_TCGA.tsv\",\n",
    "                    \"Data/Intogen_Data/Thyroid_Carcinoma_THCA_TCGA.tsv\",\n",
    "                    \"Data/Intogen_Data/Lung_Adenocarcinoma_LUAD_TCGA.tsv\",\n",
    "                    \"Data/Intogen_Data/Lung_Squamous_Cell_LUSC_TCGA.tsv\"]\n",
    "\n",
    "for path, path_intogen in zip(path_list, path_intogen_list):\n",
    "    print(path)\n",
    "    print(path_intogen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Output/Chunk_Colon.csv\n",
      "Accuracy of GBM: 1.000\n",
      "Accuracy of RFE: 0.976\n",
      "Accuracy of Elastic Net: 0.983\n",
      "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
      "Accuracy of Boruta Tree: 1.000\n",
      "Finished in 4.7 min\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation.iterate_trough_cancers(\n",
    "    [\"Output/Chunk_Colon.csv\"],\n",
    "    [\"Data/Colon_Adenocarcinoma_COREAD_TCGA.tsv\"], nrows = None, usecols = None, threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of GBM: 1.000\n",
      "Accuracy of RFE: 0.946\n",
      "Accuracy of elastic net: 0.975\n",
      "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
      "Accuracy of Boruta Tree: 1.000\n",
      "Done with: Data/Expression_Data/['Breast']_['Primary Tumor', 'Normal Tissue']_1.0.csv\n",
      "Accuracy of GBM: 1.000\n",
      "Accuracy of RFE: 0.982\n",
      "Accuracy of elastic net: 0.994\n",
      "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
      "Accuracy of Boruta Tree: 1.000\n",
      "Done with: Data/Expression_Data/['Colon']_['Primary Tumor', 'Normal Tissue']_1.0.csv\n",
      "Accuracy of GBM: 1.000\n",
      "Accuracy of RFE: 0.976\n",
      "Accuracy of elastic net: 0.989\n",
      "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
      "Accuracy of Boruta Tree: 1.000\n",
      "Done with: Data/Expression_Data/['Skin']_['Primary Tumor', 'Normal Tissue']_1.0.csv\n",
      "Accuracy of GBM: 1.000\n",
      "Accuracy of RFE: 0.976\n",
      "Accuracy of elastic net: 0.986\n",
      "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
      "Accuracy of Boruta Tree: 0.996\n",
      "Done with: Data/Expression_Data/['Thyroid', 'Thyroid Gland']_['Primary Tumor', 'Normal Tissue']_1.0.csv\n",
      "Accuracy of GBM: 0.996\n",
      "Accuracy of RFE: 0.985\n",
      "Accuracy of elastic net: 0.991\n",
      "Accuracy of Boruta: 0.999 (+/- 0.006)\n",
      "Accuracy of Boruta Tree: 1.000\n",
      "Done with: Data/Expression_Data/['Lung']_['Lung Adenocarcinoma', 'Lung']_['Primary Tumor', 'Normal Tissue']_1.0.csv\n",
      "Accuracy of GBM: 1.000\n",
      "Accuracy of RFE: 0.984\n",
      "Accuracy of elastic net: 0.989\n",
      "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
      "Accuracy of Boruta Tree: 0.992\n",
      "Done with: Data/Expression_Data/['Lung']_['Lung Squamous Cell Carcinoma', 'Lung']_['Primary Tumor', 'Normal Tissue']_1.0.csv\n"
     ]
    }
   ],
   "source": [
    "#evaluation.iterate_trough_cancers(path_list, path_intogen_list, nrows = None, usecols = None, threshold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation.iterate_trough_cancers(path_list, path_intogen_list, nrows = 300, usecols = [x for x in range(10000)], threshold = 2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1_FS_Mike_ENSG_Classes.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UjjCK1E13KGH"
   },
   "source": [
    "# Import, Set Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5UHBzLWJQPw0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import randint\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from boruta import BorutaPy\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_curve \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  C:\\Users\\micha.DESKTOP-8HA2IGV\\OneDrive\\Programming\\Propulsion Project\\intelligencia_backup\\intelligencia\n",
      "New Working Directory  C:\\Users\\micha.DESKTOP-8HA2IGV\\OneDrive\\Programming\\Propulsion Project\\intelligencia_backup\\intelligencia\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def main():\n",
    "    print(\"Current Working Directory \" , os.getcwd())\n",
    "    if os.path.exists(\"C:/Users/micha.DESKTOP-8HA2IGV/OneDrive/Programming/Propulsion Project/intelligencia_backup/intelligencia\") :\n",
    "        # Change the current working Directory    \n",
    "        os.chdir(\"C:/Users/micha.DESKTOP-8HA2IGV/OneDrive/Programming/Propulsion Project/intelligencia_backup/intelligencia\")\n",
    "        print(\"New Working Directory \" , os.getcwd())\n",
    "    else:\n",
    "        print(\"Can't change the Current Working Directory\")    \n",
    "        print(\"Current Working Directory \" , os.getcwd())\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JvF_QzfUQPxF"
   },
   "source": [
    "# Class: Data Preparation (dataprep)\n",
    "Prepares data for feature selection algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fIrp9jamvpuW"
   },
   "outputs": [],
   "source": [
    "class DataPrep:\n",
    "    \"\"\"\n",
    "    Main function -> bulbasaur(path, threshold, nrows = None, usecols = None)\n",
    "    - input: directory path of gene expression data\n",
    "    - output: X_train, y_train, x_test, y_test\n",
    "\n",
    "    Included functions:\n",
    "    - Read Data\n",
    "    - Standard Deviation Filter: Set and filter with threshold\n",
    "    - Split: Train and test split\n",
    "    - Smote Up: Upsampling to get balanced dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "    \n",
    "  # Read Data\n",
    "    def read_data(self, path, nrows, usecols):\n",
    "        data = pd.read_csv(path, nrows=nrows, usecols=usecols)\n",
    "        data.index = data.iloc[:,0]\n",
    "        data.drop(columns = \"Unnamed: 0\", inplace = True)\n",
    "        data.columns = [(re.sub('\\.\\d+', '', gene)) for gene in data.columns]\n",
    "        return data\n",
    "  \n",
    "  # Filter with Standard Deviation Threshold\n",
    "    def X_and_y(self, data, threshold):\n",
    "        #data.describe()\n",
    "        X = data.drop(columns = 'label')\n",
    "        X_sd = X.loc[:, X.std() > threshold]\n",
    "        y = data[[\"label\"]]\n",
    "        return X_sd, y\n",
    "  \n",
    "  # Train Test Split data\n",
    "    def split(self, X, y, test_size):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=self.seed)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "  \n",
    "  # Upsample unbalanced data\n",
    "    def smote_up(self, X_train, y_train):\n",
    "        #print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train['label']==1)))\n",
    "        #print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train['label']==0)))\n",
    "\n",
    "        sm = SMOTE(random_state=self.seed)\n",
    "        X_train_smote, y_train_smote = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "        #print('After OverSampling, the shape of train_X: {}'.format(X_train_smote.shape))\n",
    "        #print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_smote.shape))\n",
    "\n",
    "        #print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_smote==1)))\n",
    "        #print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_smote==0)))\n",
    "        column_names = X_train.columns\n",
    "\n",
    "        # Make dataframe again\n",
    "        X_train_smote = pd.DataFrame(X_train_smote, columns=column_names)\n",
    "        y_train_smote = pd.DataFrame(y_train_smote, columns=['label'])\n",
    "\n",
    "        return X_train_smote, y_train_smote\n",
    "    \n",
    "    def bulbasaur(self, path, threshold = 2, nrows = None, usecols = None):\n",
    "        data = self.read_data(path, nrows, usecols)\n",
    "        X, y = self.X_and_y(data, threshold)\n",
    "        X_train, X_test, y_train, y_test = self.split(X, y, 0.3)\n",
    "        X_train_smote, y_train_smote = self.smote_up(X_train, y_train)\n",
    "        return X_train_smote, y_train_smote, X_test, y_test\n",
    "    \n",
    "dataprep = DataPrep(1888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJRYDoNk3El8"
   },
   "source": [
    "# Class: Feature Selection (FS)\n",
    "Applies a variety of feature selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3eyDQGHiCts"
   },
   "outputs": [],
   "source": [
    "class FeatureSelection:\n",
    "    \"\"\"\n",
    "    Main function -> call_methods(X_train, y_train, X_test, y_test)\n",
    "    - input: Dataset splitted to train and test size.\n",
    "    - output: Dictionaries with selected features and feature importances.\n",
    "\n",
    "    Included functions:\n",
    "    - RFE: Recursive Feature Elimination\n",
    "    - Gradient Boost Classifier\n",
    "    - Elastic Net\n",
    "    - Boruta Tree: Boruta with Random Forest Classifier at the end\n",
    "    - Lasso CV: Lasso with crossvalidation\n",
    "    \"\"\"\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "    \n",
    "    def rfe(self, X_train, y_train, X_test, y_test, n_features = 300, step = 0.2, kernel = \"linear\"):\n",
    "        \"\"\"\n",
    "        Recursive Feature Elimination - step < 1 is a percentage. Returns selected features.\n",
    "        Hyperparameter tuning was done in a different notebook\n",
    "        \"\"\"\n",
    "        # Create RFE\n",
    "        estimator = SVR(kernel=kernel, C = 0.01, gamma = 1e-07)\n",
    "        selector = RFE(estimator, n_features_to_select = n_features, step=step)\n",
    "        selector = selector.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "        \n",
    "        # Print Accuracy\n",
    "        print('Accuracy of RFE: {:.3f}'.format(selector.score(X_test, y_test)))\n",
    "        \n",
    "        # Add features and feature importance to dictionary\n",
    "        selected_features = X_train.columns[selector.support_].tolist()\n",
    "        \n",
    "        feature_importances = [1 for x in range(len(selected_features))]\n",
    "    \n",
    "        dictionary = {\"Recursive Feature Elimination\":[selected_features, feature_importances]}\n",
    "        return dictionary\n",
    "  \n",
    "  \n",
    "    def gradient_boost_classifier(self, X_train, y_train, X_test, y_test, n_features = 300):\n",
    "        \"\"\"\n",
    "        Gradient Boost Classifier with feature importance selection.\n",
    "        Hypertuned parameters:\n",
    "        {'learning_rate': 0.5, 'max_depth': 3, 'min_samples_leaf': 7, 'min_samples_split': 0.1, 'n_estimators': 100, 'random_state': 1888, 'subsample': 0.75}\n",
    "        \"\"\"\n",
    "        # Create Gradient Boost Classifier \n",
    "        new = GradientBoostingClassifier(learning_rate=0.5, n_estimators=100, max_depth=3,\n",
    "                                         min_samples_split=0.1, min_samples_leaf=7,\\\n",
    "                                         subsample=0.75, random_state=self.seed)\n",
    "        new.fit(X_train,y_train)\n",
    "        predictors = list(X_train)\n",
    "        feat_imp = pd.Series(new.feature_importances_, predictors).sort_values(ascending=False)[:n_features]\n",
    "        \n",
    "        pred = new.predict(X_test)\n",
    "        \n",
    "        # Print accuracy\n",
    "        print('Accuracy of GBM: {:.3f}'.format(new.score(X_test, y_test)))\n",
    "    \n",
    "        # Add features and feature importance to dictionary\n",
    "        importances = new.feature_importances_\n",
    "        genes = X_test.columns\n",
    "        \n",
    "        selected_features_df = pd.DataFrame(importances, index = genes).sort_values(0, ascending = False).head(n_features)\n",
    "        selected_features = selected_features_df.index.tolist()\n",
    "        feature_importances = selected_features_df.iloc[:,0].tolist()\n",
    "        \n",
    "        dictionary = {\"Gradient Boost Classifier\":[selected_features, feature_importances]}\n",
    "        return dictionary\n",
    "    \n",
    "    def elastic_net(self, X_train_smote, y_train_res, X_test, y_test, alpha=0.01, l1_ratio=0.5, n_features=300):\n",
    "        clf = ElasticNet(random_state=self.seed, alpha=alpha, l1_ratio=l1_ratio)\n",
    "        clf.fit(X_train_smote, y_train_res)\n",
    "\n",
    "        clf.pred = clf.predict(X_test)\n",
    "\n",
    "        print(\"Accuracy of Elastic Net: {:.3f}\".format(clf.score(X_test, y_test)))\n",
    "        #print(\"Elastic net mean squared error: {:.3f}\".format(mean_squared_error(y_test, clf.pred)))\n",
    "\n",
    "        ft_imp = pd.DataFrame(clf.coef_, index=X_train_smote.columns)\n",
    "        ft_sort = ft_imp.sort_values(0, ascending=False)\n",
    "        imp_coef = pd.concat([ft_sort.head(int(n_features/2)), ft_sort.tail(int(n_features/2))])\n",
    "\n",
    "        selected_features = imp_coef.index.tolist()\n",
    "        feature_importances = imp_coef.iloc[:,0].tolist()\n",
    "\n",
    "        dictionary = {\"Elastic Net\": [selected_features, feature_importances]}\n",
    "\n",
    "        return dictionary \n",
    "  \n",
    "    def boruta_tree(self, X_train_smote, y_train_res, X_test, y_test):\n",
    "\n",
    "        for _ in range(1):\n",
    "\n",
    "            from sklearn.metrics import f1_score # import again because it works like that :)\n",
    "\n",
    "            # Random Forests for Boruta\n",
    "            rf_boruta = RandomForestClassifier(n_jobs=-1, random_state=self.seed)\n",
    "\n",
    "            # Perform Boruta\n",
    "            boruta = BorutaPy(rf_boruta, n_estimators='auto', verbose=0,\n",
    "                          alpha=0.005, max_iter=30, perc=100, random_state=self.seed)\n",
    "            boruta.fit(X_train_smote.values, y_train_res)\n",
    "\n",
    "            # Select features and fit Logistic Regression\n",
    "\n",
    "            cols = X_train_smote.columns[boruta.support_]\n",
    "            X_train_smote = X_train_smote[cols]\n",
    "            est_boruta = LogisticRegression(random_state=self.seed)\n",
    "            est_boruta.fit(X_train_smote, y_train_res)\n",
    "\n",
    "            scores = cross_val_score(est_boruta, X_train_smote, y_train_res, cv=5)\n",
    "\n",
    "            print(\"Accuracy of Boruta: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "        # Random Forest for extracting features\n",
    "\n",
    "        X_filtered = X_train_smote[cols]\n",
    "\n",
    "        rf = RandomForestClassifier(n_estimators = 10, criterion = 'gini', random_state = self.seed)\n",
    "        rf.fit(X_filtered, y_train_res)\n",
    "        rf_pred = rf.predict(X_test[cols])\n",
    "        print(\"Accuracy of Boruta Tree: {:.3f}\".format(accuracy_score(y_test, rf_pred)))\n",
    "\n",
    "        feature_names = X_filtered.columns\n",
    "        rf_coeff = pd.DataFrame({\"feature\": feature_names,\"coefficient\": rf.feature_importances_})\n",
    "        rf_coeff_top = rf_coeff.sort_values(by=\"coefficient\",ascending=False).head(300).set_index(\"feature\")\n",
    "\n",
    "        selected_features = rf_coeff_top.index.tolist()\n",
    "        feature_importances = rf_coeff_top.coefficient.tolist()\n",
    "\n",
    "        dictionary = {\"Boruta Tree\": [selected_features, feature_importances]}\n",
    "\n",
    "        return dictionary\n",
    "    \n",
    "    def lasso_cv(self, X_train, y_train, X_test, y_test, n_features = 300):\n",
    "    # LassoCV\n",
    "        \n",
    "        lassoCV = LassoCV(cv=3, random_state=1888).fit(X_train, y_train)\n",
    "        print(\"Accuracy of LassoCV {:.3f}\".format(lassoCV.score(X_test, y_test)))\n",
    "\n",
    "        def imp_coef(model, n, columns=X_train.columns):\n",
    "            array_to_df = pd.DataFrame(model.coef_)\n",
    "            array_to_df.index = columns\n",
    "            array_sorted = array_to_df.sort_values(0, ascending=False)\n",
    "            imp_coef = pd.concat([array_sorted.head(int(n/2)), array_sorted.tail(int(n/2))])\n",
    "\n",
    "            feature_importances = imp_coef.iloc[:,0].tolist()\n",
    "            selected_features = imp_coef.index.tolist()\n",
    "            return {\"Lasso CV\": [selected_features, feature_importances]}\n",
    "        \n",
    "        dict_ = imp_coef(lassoCV, n = n_features)\n",
    "        return dict_\n",
    "        \n",
    "    def call_methods(self, X_train, y_train, X_test, y_test):\n",
    "        method1 = self.gradient_boost_classifier(X_train, y_train, X_test, y_test, n_features = 300)\n",
    "        method2 = self.rfe(X_train, y_train, X_test, y_test, kernel = \"linear\")\n",
    "        method3 = self.elastic_net(X_train, y_train, X_test, y_test, alpha=0.01, l1_ratio=0.5, n_features = 300)\n",
    "        method4 = self.boruta_tree(X_train, y_train, X_test, y_test)\n",
    "        method5 = self.lasso_cv(X_train, y_train, X_test, y_test, n_features = 300)\n",
    "\n",
    "        return { **method1, **method2, **method3, **method4, **method5}\n",
    "        \n",
    "# Instantiate Class Object\n",
    "FS = FeatureSelection(1888)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class: Evaluation (evaluation)\n",
    "Stores results (Counts of genes selected, importances, overlaps with cosmic) in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluation:\n",
    "    \"\"\"\n",
    "    Included functions:\n",
    "    - Add Cosmic to Dict: Add a dictionary with Cosmic cancer-related genes.\n",
    "    - Results: Store results as csv file\n",
    "    - Normalize Importances: Normalize of importances and add column with Total Importance\n",
    "    - Final Results: Everything together in one df :)\n",
    "    - Iterate Through Cancers: Iterate throuhg all cancer data\n",
    "    \"\"\"    \n",
    "    def add_cosmic_to_dict(self, path, dict_list):\n",
    "        \"\"\"\n",
    "        Add a dictionary with Cosmic cancer-related genes and corresponding Mutation Count\n",
    "        \"\"\"\n",
    "        #path = \"Data/Intogen_Data/Lung_Adenocarcinoma_LUAD_TCGA.tsv\"\n",
    "        census = pd.read_csv(\"Data/Reference_Data/Census_allWed May 15 09_46_55 2019.csv\")\n",
    "        census[\"GENE\"] = census[\"Synonyms\"].str.extract(pat = '(ENSG...........)')\n",
    "        census = census[census[\"GENE\"].notnull()]\n",
    "        census.fillna(\"None\", inplace = True)\n",
    "        importances = census[\"Role in Cancer\"].tolist()\n",
    "        cosmic_genes = {\"Cosmic\":[census[\"GENE\"].to_list(), importances]}\n",
    "        dict_list = {**dict_list, **cosmic_genes}\n",
    "        \n",
    "        return dict_list\n",
    "    \n",
    "    def results(self, dict_list):\n",
    "        \"\"\"\n",
    "        Store results in a csv\n",
    "        Includes: Count for each method, feature importance, intogen counts\n",
    "        \"\"\"\n",
    "        row_names = []\n",
    "        column_names = []\n",
    "\n",
    "        # Create dataframe with all viable genes from all method results\n",
    "        for method, selected_features in dict_list.items():\n",
    "            \n",
    "            for feature in selected_features[0]:\n",
    "                row_names.append(feature)\n",
    "\n",
    "            row_names = list(set(row_names))\n",
    "            column_names.append(method)\n",
    "\n",
    "        results = pd.DataFrame(columns = column_names, index = row_names)\n",
    "        results.fillna(0, inplace = True)\n",
    "        \n",
    "        # Add a one where the method selected the corresponding feature\n",
    "        for method, selected_features in dict_list.items():\n",
    "            for feature in selected_features[0]:\n",
    "                results.at[feature, method] = 1\n",
    "\n",
    "        # Create Column with total count\n",
    "        results['Total Count'] = results[list(results.columns)].sum(axis=1)\n",
    "        results.sort_values(by = \"Total Count\", ascending = False, inplace = True)\n",
    "        \n",
    "        # Add Importance Columns\n",
    "        for method, selected_features in dict_list.items():\n",
    "            additional = pd.DataFrame({\"Importances: \" + method:selected_features[1]}, index = selected_features[0])\n",
    "            results = results.join(additional, how=\"outer\")\n",
    "            \"\"\"if \"key_0\" in results.columns:\n",
    "                results.drop(columns = \"key_0\", inplace = True)\"\"\"\n",
    "        \n",
    "        # Clean dataframe\n",
    "        results.fillna(0, inplace = True)\n",
    "        results = results.reset_index().drop_duplicates(subset='index', keep='first').set_index('index')\n",
    "        results.rename(index=str, columns={\"Importances: Cosmic\": \"Role in Cancer\"}, inplace = True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def normalize_importance(self, result, threshold = 0):\n",
    "        \"\"\"\n",
    "        This function returns the results table with normalized importances and an extra column - Total importance\n",
    "        It is then sorted by total importance\n",
    "        \"\"\"\n",
    "        #Normalize Importances\n",
    "        imp = ['Importances: Gradient Boost Classifier',\n",
    "               'Importances: Recursive Feature Elimination',\n",
    "               'Importances: Elastic Net', \n",
    "               'Importances: Boruta Tree',\n",
    "               'Importances: Lasso CV']\n",
    "        \n",
    "        scaler1 = MinMaxScaler() \n",
    "        scaler2 = StandardScaler(copy=True, with_mean=True, with_std=True)\n",
    "        \n",
    "        result[imp] = result[imp].abs()\n",
    "        scaled_values = scaler1.fit_transform(result[imp]) \n",
    "        scaled_values = scaler2.fit_transform(scaled_values)\n",
    "        \n",
    "        result[imp] = scaled_values\n",
    "        \n",
    "        result['Importances: Recursive Feature Elimination'] = result['Importances: Recursive Feature Elimination']/4\n",
    "        \n",
    "        result[\"Importance Score\"] = result[imp].sum(axis=1)\n",
    "        result[\"Importance Score\"] = scaler1.fit_transform(result[[\"Importance Score\"]])\n",
    "        result = result[result[\"Total Count\"] > threshold].sort_values(by = \"Importance Score\", ascending = False)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def final_results(self, path, path_intogen, nrows = 200, usecols = [x for x in range(100)], threshold = 3):\n",
    "        \"\"\"\n",
    "        Puts everything together\n",
    "        \"\"\"\n",
    "        X_train, y_train, X_test, y_test = dataprep.bulbasaur(path, threshold, nrows = nrows, usecols = usecols)\n",
    "        dict_list = FS.call_methods(X_train, y_train, X_test, y_test)\n",
    "        dict_list = evaluation.add_cosmic_to_dict(path_intogen, dict_list)\n",
    "        df = evaluation.results(dict_list)\n",
    "        df = evaluation.normalize_importance(df)\n",
    "        return df\n",
    "\n",
    "    def iterate_trough_cancers(self, path_list, path_intogen_list, nrows, usecols, threshold = 2.5):\n",
    "        \"\"\"\n",
    "        Iterate through all cancer data\n",
    "        \"\"\"\n",
    "        for path, path_intogen in zip(path_list, path_intogen_list):\n",
    "            start = time.time()\n",
    "            name = re.sub(\"^.+\\/Chunk_\", \"2.0_\", path)\n",
    "            filepath = 'Output/Results/Result_{}'.format(name)\n",
    "\n",
    "            print('Evaluating {}'.format(path))      \n",
    "\n",
    "            results = evaluation.final_results(path, path_intogen, nrows = nrows, usecols = usecols, threshold = threshold)  \n",
    "            results.to_csv(filepath)\n",
    "\n",
    "            print('Finished in {:.1f} min\\n'.format((time.time() - start) / 60))\n",
    "\n",
    "evaluation = Evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_intogen_list = [\"Data/Reference_Data/Census_allWed May 15 09_46_55 2019.csv\"]#*7\n",
    "path_list = [\"Output/Chunk_Breast.csv\",\n",
    "            \"Output/Chunk_LungAdenocarcinoma_Lung.csv\",\n",
    "            \"Output/Chunk_LungSquamousCellCarcinoma_Lung.csv\",\n",
    "            \"Output/Chunk_Skin.csv\",\n",
    "            \"Output/Chunk_Thyroid_ThyroidGland.csv\",\n",
    "            \"Output/Chunk_LungA1_vs_LungS1.csv\",\n",
    "            \"Output/Chunk_AllCancers_0vs1.csv\",\n",
    "            \"Output/Chunk_Colon.csv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation.iterate_trough_cancers(path_list, path_intogen_list, nrows = None, usecols = None, threshold = 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Model Accuracies (threshold = 1, all rows and cols):\n",
    "\n",
    "Evaluating Output/Chunk_Breast.csv\n",
    "Accuracy of GBM: 0.997\n",
    "Accuracy of RFE: 0.928\n",
    "Accuracy of Elastic Net: 0.950\n",
    "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
    "Accuracy of Boruta Tree: 1.000\n",
    "Accuracy of LassoCV 0.957\n",
    "Finished in 21.8 min\n",
    "\n",
    "Evaluating Output/Chunk_LungAdenocarcinoma_Lung.csv\n",
    "Accuracy of GBM: 1.000\n",
    "Accuracy of RFE: 0.971\n",
    "Accuracy of Elastic Net: 0.969\n",
    "Accuracy of Boruta: 0.999 (+/- 0.006)\n",
    "Accuracy of Boruta Tree: 1.000\n",
    "Accuracy of LassoCV 0.972\n",
    "Finished in 8.4 min\n",
    "\n",
    "Evaluating Output/Chunk_LungSquamousCellCarcinoma_Lung.csv\n",
    "Accuracy of GBM: 0.996\n",
    "Accuracy of RFE: 0.971\n",
    "Accuracy of Elastic Net: 0.976\n",
    "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
    "Accuracy of Boruta Tree: 0.992\n",
    "Accuracy of LassoCV 0.976\n",
    "Finished in 14.9 min\n",
    "\n",
    "Evaluating Output/Chunk_Skin.csv\n",
    "Accuracy of GBM: 0.990\n",
    "Accuracy of RFE: 0.980\n",
    "Accuracy of Elastic Net: 0.981\n",
    "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
    "Accuracy of Boruta Tree: 1.000\n",
    "Accuracy of LassoCV 0.981\n",
    "Finished in 8.9 min\n",
    "\n",
    "Evaluating Output/Chunk_Thyroid_ThyroidGland.csv\n",
    "Accuracy of GBM: 0.996\n",
    "Accuracy of RFE: 0.962\n",
    "Accuracy of Elastic Net: 0.964\n",
    "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
    "Accuracy of Boruta Tree: 0.996\n",
    "Accuracy of LassoCV 0.964\n",
    "Finished in 10.9 min\n",
    "\n",
    "Evaluating Output/Chunk_LungA1_vs_LungS1.csv\n",
    "Accuracy of GBM: 0.934\n",
    "Accuracy of RFE: 0.774\n",
    "Accuracy of Elastic Net: 0.817\n",
    "Accuracy of Boruta: 0.957 (+/- 0.031)\n",
    "Accuracy of Boruta Tree: 0.951\n",
    "Accuracy of LassoCV 0.832\n",
    "Finished in 12.5 min\n",
    "\n",
    "Evaluating Output/Chunk_AllCancers_0vs1.csv\n",
    "Accuracy of GBM: 1.000\n",
    "Accuracy of RFE: 0.975\n",
    "Accuracy of Elastic Net: 0.983\n",
    "Accuracy of Boruta: 1.000 (+/- 0.000)\n",
    "Accuracy of Boruta Tree: 0.999\n",
    "Accuracy of LassoCV 0.984\n",
    "Finished in 101.4 min"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "1_FS_Mike_ENSG_Classes.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
